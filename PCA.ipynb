{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis (for authorship verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple implementation of Principal Components Analysis in analysing authorship, using 19 measurements from each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "from io import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set directory for text input files\n",
    "text_files = os.path.join(os.path.dirname(\"__file__\"), 'texts')\n",
    "files = os.listdir(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lists for storing data for PCA processing\n",
    "X = []  # Stores data on each input text\n",
    "y = []  # Stores authors of input texts\n",
    "markers = []  # Stores only unique author names for plotting purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to collect relevant data from text \n",
    "\n",
    "<b>N.B. Code was written separately so will likely need rewritten</b>\n",
    "\n",
    "Relevant data: (see Hanlein, H. “Studies in Authorship Recognition: a Corpus-based Approach”)\n",
    "<ul>\n",
    "<li>Mean word length</li>\n",
    "<li>Mean sentence length</li>\n",
    "<li>Standard deviation of sentence length</li>\n",
    "<li>Number of unique words in a text (Type Token Ratio)</li>\n",
    "<li>Instances of ',' per 1000 tokens</li>\n",
    "<li>Instances of ';' per 1000 tokens</li>\n",
    "<li>Instances of '\"' per 1000 tokens</li>\n",
    "<li>Instances of '!' per 1000 tokens</li>\n",
    "<li>Instances of '-' per 1000 tokens</li>\n",
    "<li>Instances of 'and' per 1000 tokens</li>\n",
    "<li>Instances of 'but' per 1000 tokens</li>\n",
    "<li>Instances of 'however' per 1000 tokens</li>\n",
    "<li>Instances of 'if' per 1000 tokens</li>\n",
    "<li>Instances of 'that' per 1000 tokens</li>\n",
    "<li>Instances of 'more' per 1000 tokens</li>\n",
    "<li>Instances of 'must' per 1000 tokens</li>\n",
    "<li>Instances of 'might' per 1000 tokens</li>\n",
    "<li>Instances of 'this' per 1000 tokens</li>\n",
    "<li>Instances of 'very' per 1000 tokens</li>\n",
    "</ul>\n",
    "\n",
    "In addition, it is suggested that chapter length and mean paragraph length should be considered. Given the goal of this project, these are likely to be irrelevant. (Although possibly chapter length becomes stanza length - context dependent)\n",
    "\n",
    "- TODO will research additional possibilities - w/ emphasis on Scots language determiners/superlatives\n",
    "- Include some sort of bigram calculation, eg. no. of unique bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from numpy import std\n",
    "from collections import Counter\n",
    "\n",
    "common_elems = [',', ';', '\"', '!', '-', 'and', 'but', 'however',\n",
    "                'if', 'that', 'more', 'must', 'might', 'this', 'very']\n",
    "\n",
    "\n",
    "def initial_read(text):\n",
    "    \"\"\"Reads the text file and saves it as a string\"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    data = list()\n",
    "    data.append(mean_word_length(words))\n",
    "    data.append(mean_sentence_length(sentences))\n",
    "    data.append(sd_of_sentence_length(sentences))\n",
    "    data.append(type_token_ratio(words))\n",
    "    \n",
    "# CODE BELOW APPENDS NUMBER OF UNIQUE BIGRAMS PER TEXT, NOT SURE OF THE VALUE OF THIS AS YET, WILL RESEARCH FURTHER\n",
    "# Brings some texts closer together\n",
    "\n",
    "    from nltk import bigrams\n",
    "    bg = bigrams(words)\n",
    "    bg = set(bg)\n",
    "    bg = Counter(bg)\n",
    "\n",
    "    data.append(float(len(bg)) / len(words))\n",
    "\n",
    "    for word in common_elems:\n",
    "        data.append(count_words(text, word, len(words)))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def mean_word_length(words):\n",
    "    \"\"\"Calculates mean length of words in a text\"\"\"\n",
    "    total_length = 0\n",
    "    for word in words:\n",
    "        total_length += len(word)\n",
    "    return total_length/len(words)\n",
    "\n",
    "\n",
    "def mean_sentence_length(sentences):\n",
    "    \"\"\"Calculates the mean length of each sentence\"\"\"\n",
    "    total_length = 0\n",
    "    for sentence in sentences:\n",
    "        total_length += len(sentence)\n",
    "\n",
    "    mean = total_length/len(sentences)\n",
    "    return mean\n",
    "\n",
    "\n",
    "def sd_of_sentence_length(sentences):\n",
    "    \"\"\"Returns the standard deviation in sentence length\"\"\"\n",
    "    sentence_lengths = []\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        sentence_lengths.append(sentence_length)\n",
    "\n",
    "    sd = std(sentence_lengths)\n",
    "    return sd\n",
    "\n",
    "\n",
    "def count_words(text, word, wordcount):\n",
    "    \"\"\"Returns the count of a given word/character per 1000 words\"\"\"\n",
    "    total = text.count(word)\n",
    "    thousands = wordcount / 500.0\n",
    "    total /= thousands\n",
    "    return total\n",
    "\n",
    "\n",
    "def type_token_ratio(words):\n",
    "    \"\"\"Calculates the type token ratio of a text\"\"\"\n",
    "    c = Counter(words)\n",
    "    # TTR is the number of unique words in a text divided by the total word count\n",
    "    #ttr = float(len(c)) / len(words)\n",
    "    ttr = float(len(c)) / 500\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data\n",
    "\n",
    "Novels by: <ul>\n",
    "    <li>Jane Austen</li>\n",
    "    <li>Walter Scott</li>\n",
    "    <li>Charles Dickens</li>\n",
    "    <li>Rudyard Kipling</li>\n",
    "    <li>Henry James</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through each text file and append relevant data to lists\n",
    "for doc in os.listdir(text_files):\n",
    "\n",
    "    text = open(text_files + '\\\\' + doc, 'r', encoding='utf-8')\n",
    "    # open the file, store as lower case\n",
    "    text = text.read().lower()\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    X.append(initial_read(text))\n",
    "    \n",
    "    # Right now txt files are named \"author <number>.txt\", redesign?\n",
    "    author = doc.split()\n",
    "\n",
    "    y.append(author[0].title())\n",
    "    if author[0].title() not in markers:\n",
    "        markers.append(author[0].title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### Convert both X and Y to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardise data, perform PCA\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~daveSHMB/123.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.tools as tls\n",
    "\n",
    "traces = []\n",
    "\n",
    "for author in markers:\n",
    "\n",
    "    trace = Scatter(\n",
    "        x=Y_sklearn[y==author,0],\n",
    "        y=Y_sklearn[y==author,1],\n",
    "        mode='markers',\n",
    "        name=author,\n",
    "        marker=Marker(\n",
    "            size=14))\n",
    "    traces.append(trace)\n",
    "\n",
    "\n",
    "data = Data(traces)\n",
    "layout = Layout(title='PCA of 5 authors',\n",
    "                xaxis=XAxis(title='Principal Component 1'),\n",
    "                yaxis=YAxis(title='Principal Component 2'))\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get stats on Burns most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "burns = ''\n",
    "\n",
    "# Loop through each text file and append relevant data to lists\n",
    "for doc in os.listdir(text_files):\n",
    "\n",
    "    text = open(text_files + '\\\\' + doc, 'r', encoding='utf-8')\n",
    "    # open the file, store as lower case\n",
    "    text = text.read().lower()\n",
    "    \n",
    "    burns += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def common_words(words):\n",
    "    \n",
    "    import regex as re\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    from nltk.tokenize import wordpunct_tokenize\n",
    "    import string\n",
    "\n",
    "    words = wordpunct_tokenize(words)\n",
    "#     remove punctuation\n",
    "#     words = re.sub(ur\"\\p{P}+\", \"\", words)\n",
    "    c = Counter(words)\n",
    "    \n",
    "    for p in string.punctuation:\n",
    "        del c[p]\n",
    "    \n",
    "    from IPython.display import display, HTML\n",
    "    print(\"Most common Burns words (including stopwords):\")\n",
    "    df = pd.DataFrame.from_dict(c.most_common(25))\n",
    "    df.columns = ['Word', 'Frequency']\n",
    "    display(df)\n",
    "\n",
    "    # remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    for word in stopwords.words('english'):\n",
    "        del c[word]\n",
    "        \n",
    "     \n",
    "    print(\"Most common Burns words (without stopwords):\")\n",
    "    df = pd.DataFrame.from_dict(c.most_common(25))\n",
    "    df.columns = ['Word', 'Frequency']\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common Burns words (including stopwords):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>26634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>15308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>9050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>8848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>8572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s</td>\n",
       "      <td>6787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>in</td>\n",
       "      <td>6356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i</td>\n",
       "      <td>6310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>that</td>\n",
       "      <td>5036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>his</td>\n",
       "      <td>4482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>o</td>\n",
       "      <td>4299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>my</td>\n",
       "      <td>3954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>he</td>\n",
       "      <td>3803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>d</td>\n",
       "      <td>3546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>for</td>\n",
       "      <td>3514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>but</td>\n",
       "      <td>3235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>her</td>\n",
       "      <td>3136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>on</td>\n",
       "      <td>3060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>with</td>\n",
       "      <td>2961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>was</td>\n",
       "      <td>2819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>an</td>\n",
       "      <td>2695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>as</td>\n",
       "      <td>2340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>is</td>\n",
       "      <td>2282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>it</td>\n",
       "      <td>2232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>they</td>\n",
       "      <td>2032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word  Frequency\n",
       "0    the      26634\n",
       "1    and      15308\n",
       "2     to       9050\n",
       "3     of       8848\n",
       "4      a       8572\n",
       "5      s       6787\n",
       "6     in       6356\n",
       "7      i       6310\n",
       "8   that       5036\n",
       "9    his       4482\n",
       "10     o       4299\n",
       "11    my       3954\n",
       "12    he       3803\n",
       "13     d       3546\n",
       "14   for       3514\n",
       "15   but       3235\n",
       "16   her       3136\n",
       "17    on       3060\n",
       "18  with       2961\n",
       "19   was       2819\n",
       "20    an       2695\n",
       "21    as       2340\n",
       "22    is       2282\n",
       "23    it       2232\n",
       "24  they       2032"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common Burns words (without stopwords):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>—</td>\n",
       "      <td>1944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>er</td>\n",
       "      <td>1783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thy</td>\n",
       "      <td>1718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ye</td>\n",
       "      <td>1665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wi</td>\n",
       "      <td>1502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>thou</td>\n",
       "      <td>1394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>like</td>\n",
       "      <td>1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>may</td>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>man</td>\n",
       "      <td>1058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>heart</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>thee</td>\n",
       "      <td>958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>shall</td>\n",
       "      <td>917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>day</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>love</td>\n",
       "      <td>868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>still</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>e</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>never</td>\n",
       "      <td>733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>see</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sae</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>yet</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>come</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>one</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>till</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>night</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>fair</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Frequency\n",
       "0       —       1944\n",
       "1      er       1783\n",
       "2     thy       1718\n",
       "3      ye       1665\n",
       "4      wi       1502\n",
       "5    thou       1394\n",
       "6    like       1128\n",
       "7     may       1103\n",
       "8     man       1058\n",
       "9   heart        969\n",
       "10   thee        958\n",
       "11  shall        917\n",
       "12    day        903\n",
       "13   love        868\n",
       "14  still        818\n",
       "15      e        752\n",
       "16  never        733\n",
       "17    see        710\n",
       "18    sae        704\n",
       "19    yet        682\n",
       "20   come        610\n",
       "21    one        594\n",
       "22   till        593\n",
       "23  night        572\n",
       "24   fair        567"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_words(burns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
