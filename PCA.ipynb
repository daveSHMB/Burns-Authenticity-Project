{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis (for authorship verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple implementation of Principal Components Analysis in analysing authorship, using 19 measurements from each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "from io import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set directory for text input files\n",
    "text_files = os.path.join(os.path.dirname(\"__file__\"), 'texts')\n",
    "files = os.listdir(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lists for storing data for PCA processing\n",
    "X = []  # Stores data on each input text\n",
    "y = []  # Stores authors of input texts\n",
    "markers = []  # Stores only unique author names for plotting purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to collect relevant data from text \n",
    "\n",
    "<b>N.B. Code was written separately so will likely need rewritten</b>\n",
    "\n",
    "Relevant data: (see Hanlein, H. “Studies in Authorship Recognition: a Corpus-based Approach”)\n",
    "<ul>\n",
    "<li>Mean word length</li>\n",
    "<li>Mean sentence length</li>\n",
    "<li>Standard deviation of sentence length</li>\n",
    "<li>Number of unique words in a text (Type Token Ratio)</li>\n",
    "<li>Instances of ',' per 1000 tokens</li>\n",
    "<li>Instances of ';' per 1000 tokens</li>\n",
    "<li>Instances of '\"' per 1000 tokens</li>\n",
    "<li>Instances of '!' per 1000 tokens</li>\n",
    "<li>Instances of '-' per 1000 tokens</li>\n",
    "<li>Instances of 'and' per 1000 tokens</li>\n",
    "<li>Instances of 'but' per 1000 tokens</li>\n",
    "<li>Instances of 'however' per 1000 tokens</li>\n",
    "<li>Instances of 'if' per 1000 tokens</li>\n",
    "<li>Instances of 'that' per 1000 tokens</li>\n",
    "<li>Instances of 'more' per 1000 tokens</li>\n",
    "<li>Instances of 'must' per 1000 tokens</li>\n",
    "<li>Instances of 'might' per 1000 tokens</li>\n",
    "<li>Instances of 'this' per 1000 tokens</li>\n",
    "<li>Instances of 'very' per 1000 tokens</li>\n",
    "</ul>\n",
    "\n",
    "In addition, it is suggested that chapter length and mean paragraph length should be considered. Given the goal of this project, these are likely to be irrelevant. (Although possibly chapter length becomes stanza length - context dependent)\n",
    "\n",
    "- TODO will research additional possibilities - w/ emphasis on Scots language determiners/superlatives\n",
    "- Include some sort of bigram calculation, eg. no. of unique bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from numpy import std\n",
    "from collections import Counter\n",
    "\n",
    "common_elems = [',', ';', '\"', '!', '-', 'and', 'but', 'however',\n",
    "                'if', 'that', 'more', 'must', 'might', 'this', 'very']\n",
    "\n",
    "\n",
    "def initial_read(text):\n",
    "    \"\"\"Reads the text file and saves it as a string\"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    data = list()\n",
    "    data.append(mean_word_length(words))\n",
    "    data.append(mean_sentence_length(sentences))\n",
    "    data.append(sd_of_sentence_length(sentences))\n",
    "    data.append(type_token_ratio(words))\n",
    "    \n",
    "# CODE BELOW APPENDS NUMBER OF UNIQUE BIGRAMS PER TEXT, NOT SURE OF THE VALUE OF THIS AS YET, WILL RESEARCH FURTHER\n",
    "# Brings some texts closer together\n",
    "\n",
    "#     from nltk import bigrams\n",
    "#     bg = bigrams(words)\n",
    "#     bg = set(bg)\n",
    "#     bg = Counter(bg)\n",
    "#     data.append(float(len(bg)) / len(words))\n",
    "\n",
    "    for word in common_elems:\n",
    "        data.append(count_words(text, word, len(words)))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def mean_word_length(words):\n",
    "    \"\"\"Calculates mean length of words in a text\"\"\"\n",
    "    total_length = 0\n",
    "    for word in words:\n",
    "        total_length += len(word)\n",
    "    return total_length/len(words)\n",
    "\n",
    "\n",
    "def mean_sentence_length(sentences):\n",
    "    \"\"\"Calculates the mean length of each sentence\"\"\"\n",
    "    total_length = 0\n",
    "    for sentence in sentences:\n",
    "        total_length += len(sentence)\n",
    "\n",
    "    mean = total_length/len(sentences)\n",
    "    return mean\n",
    "\n",
    "\n",
    "def sd_of_sentence_length(sentences):\n",
    "    \"\"\"Returns the standard deviation in sentence length\"\"\"\n",
    "    sentence_lengths = []\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        sentence_lengths.append(sentence_length)\n",
    "\n",
    "    sd = std(sentence_lengths)\n",
    "    return sd\n",
    "\n",
    "\n",
    "def count_words(text, word, wordcount):\n",
    "    \"\"\"Returns the count of a given word/character per 1000 words\"\"\"\n",
    "    total = text.count(word)\n",
    "    thousands = wordcount / 1000.0\n",
    "    total /= thousands\n",
    "    return total\n",
    "\n",
    "\n",
    "def type_token_ratio(words):\n",
    "    \"\"\"Calculates the type token ratio of a text\"\"\"\n",
    "    c = Counter(words)\n",
    "    # TTR is the number of unique words in a text divided by the total word count\n",
    "    ttr = float(len(c)) / len(words)\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data\n",
    "\n",
    "Novels by: <ul>\n",
    "    <li>Jane Austen</li>\n",
    "    <li>Walter Scott</li>\n",
    "    <li>Charles Dickens</li>\n",
    "    <li>Rudyard Kipling</li>\n",
    "    <li>Henry James</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through each text file and append relevant data to lists\n",
    "for doc in os.listdir(text_files):\n",
    "\n",
    "    text = open(text_files + '\\\\' + doc, 'r', encoding='utf-8')\n",
    "    # open the file, store as lower case\n",
    "    text = text.read().lower()\n",
    "\n",
    "    X.append(initial_read(text))\n",
    "    \n",
    "    # Right now txt files are named \"author <number>.txt\", redesign?\n",
    "    author = doc.split()\n",
    "\n",
    "    y.append(author[0].title())\n",
    "    if author[0].title() not in markers:\n",
    "        markers.append(author[0].title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Convert both X and Y to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardise data, perform PCA\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~daveSHMB/36.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.tools as tls\n",
    "\n",
    "traces = []\n",
    "\n",
    "for author in markers:\n",
    "\n",
    "    trace = Scatter(\n",
    "        x=Y_sklearn[y==author,0],\n",
    "        y=Y_sklearn[y==author,1],\n",
    "        mode='markers',\n",
    "        name=author,\n",
    "        marker=Marker(\n",
    "            size=14))\n",
    "    traces.append(trace)\n",
    "\n",
    "\n",
    "data = Data(traces)\n",
    "layout = Layout(title='PCA of 5 authors',\n",
    "                xaxis=XAxis(title='Principal Component 1'),\n",
    "                yaxis=YAxis(title='Principal Component 2'))\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
