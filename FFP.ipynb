{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from io import open\n",
    "\n",
    "# set directory for text input files\n",
    "text_files = os.path.join(os.path.dirname(\"__file__\"), 'texts')\n",
    "files = os.listdir(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "authors = []\n",
    "\n",
    "# Loop through each text file and append relevant data to lists\n",
    "for doc in os.listdir(text_files):\n",
    "\n",
    "    text = open(text_files + '\\\\' + doc, 'r', encoding='utf-8')\n",
    "    # open the file, store as lower case\n",
    "    text = text.read().lower()\n",
    "    texts.append(text)\n",
    "    \n",
    "    authors.append(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 10  #ngram size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_grams = set([])\n",
    "treated_texts = []\n",
    "\n",
    "import regex as re\n",
    "import string\n",
    "\n",
    "for text in texts:\n",
    "    \n",
    "    # Remove high frequency stop words, as in Sims/Wu et. al\n",
    "    stopwords = ['and', 'the', 'a', 'an']\n",
    "    #text = text.split()\n",
    "    \n",
    "    # removes all punctuation, spaces and new lines    \n",
    "    #text = re.sub(ur\"\\p{P}+\", \"\", text)\n",
    "    \n",
    "    # Split text, remove stopwords and rejoin string without spaces or newlines\n",
    "    text = text.split()\n",
    "\n",
    "    text = [x for x in text if x not in stopwords]\n",
    "    \n",
    "    text = ''.join(text)   \n",
    "    \n",
    "    #produces 6 character n-grams in a 'sliding window' across the text\n",
    "    text = [text[i:i+n] for i in range(0, len(text), 1) if not any(j in string.punctuation for j in text[i:i+n])]\n",
    "    treated_texts.append(text)   \n",
    "    #n_gram_frequency = Counter(text)\n",
    "    \n",
    "    # Append features to the ngram list\n",
    "    from collections import Counter\n",
    "    c = Counter(text)\n",
    "    \n",
    "   \n",
    "\n",
    "    # Append features to the ngram list\n",
    "    for key, val in c.iteritems():\n",
    "        #if not any(i in string.punctuation for i in key)\n",
    "        n_grams.add(key)\n",
    "        \n",
    "           \n",
    "                \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "913408\n"
     ]
    }
   ],
   "source": [
    "print len(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "n_grams = OrderedDict.fromkeys(n_grams, 0.0) \n",
    "\n",
    "frequencies = []\n",
    "\n",
    "t = []\n",
    "\n",
    "for text in treated_texts:\n",
    "    c = Counter(text)\n",
    "    temp = OrderedDict(n_grams)\n",
    "    \n",
    "    for key, val in c.iteritems():\n",
    "        if len(key) == n:\n",
    "            temp[key] = float(val)\n",
    "\n",
    "    frequencies.append(temp.values()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def JSD(P, Q):\n",
    "    P = np.array(P, dtype = 'float_')\n",
    "    Q = np.array(Q, dtype = 'float_')\n",
    "    P = P / np.sum(P)\n",
    "    Q = Q / np.sum(Q)\n",
    "    M = (P + Q) / 2\n",
    "    jsd = 0.5 * (entropy(P, M) + entropy(Q, M))\n",
    "    return jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO - make this more efficient\n",
    "\n",
    "distance_matrix = [[] for x in range(len(frequencies))]\n",
    "\n",
    "for i in range(len(frequencies)):\n",
    "    for j in range(len(frequencies)):\n",
    "        \n",
    "        if i==j:\n",
    "            distance_matrix[i].append(0)\n",
    "            break\n",
    "        \n",
    "        distance_matrix[i].append(JSD(frequencies[i], frequencies[j]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  _________________________________________________ wilson 1.txt\n",
      " |\n",
      " |__________________________________________________ fergusson 1.txt\n",
      " |\n",
      " |__________________________________________________ hogg 3.txt\n",
      "_|\n",
      " | _________________________________________________ hogg 4.txt\n",
      " ||\n",
      " ||_________________________________________________ hogg 2.txt\n",
      " ||\n",
      " ||_________________________________________________ hogg 1.txt\n",
      " |\n",
      " |__________________________________________________ tannahill 1.txt\n",
      " |\n",
      " | __________________________________________________ Burns4.txt\n",
      " ||\n",
      " ,|             _____________________________________ Burns3.txt\n",
      " || ___________|\n",
      " |||           |____________________________________ Burns2.txt\n",
      " | |\n",
      " | |_________________________________________________ 10randomBurnspoems.txt\n",
      " |\n",
      " |____________________________________________________ B-List 1.txt\n",
      " |\n",
      " |___________________________________________________ A-List 1.txt\n",
      " |\n",
      " |__________________________________________________ Burns1.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Bio.Phylo.TreeConstruction import DistanceTreeConstructor\n",
    "from Bio.Phylo.TreeConstruction import _DistanceMatrix \n",
    "from Bio import Phylo\n",
    "from pygraphviz import *\n",
    "\n",
    "\n",
    "m = _DistanceMatrix(authors, distance_matrix)\n",
    "\n",
    "\n",
    "constructor = DistanceTreeConstructor()\n",
    "\n",
    "njtree = constructor.nj(m)\n",
    "Phylo.write(njtree, '10', 'newick')\n",
    "# njtree = Phylo.read('treefile', 'newick')\n",
    "Phylo.draw_ascii(njtree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # PCA using the ngram frequencies, just for laughs\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# ###### Convert both X and Y to numpy arrays\n",
    "# X = np.array(frequencies)\n",
    "# y = np.array(authors)\n",
    "# markers = set(authors)\n",
    "\n",
    "# # Standardise data, perform PCA\n",
    "# from sklearn.decomposition import PCA as sklearnPCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# X_std = StandardScaler().fit_transform(X)\n",
    "# sklearn_pca = sklearnPCA(n_components=2)\n",
    "# Y_sklearn = sklearn_pca.fit_transform(X_std)\n",
    "\n",
    "# import plotly.plotly as py\n",
    "# from plotly.graph_objs import *\n",
    "# import plotly.tools as tls\n",
    "\n",
    "# traces = []\n",
    "\n",
    "# for author in markers:\n",
    "\n",
    "#     trace = Scatter(\n",
    "#         x=Y_sklearn[y==author,0],\n",
    "#         y=Y_sklearn[y==author,1],\n",
    "#         mode='markers',\n",
    "#         name=author,\n",
    "#         marker=Marker(\n",
    "#             size=14))\n",
    "#     traces.append(trace)\n",
    "\n",
    "\n",
    "# data = Data(traces)\n",
    "# layout = Layout(title='PCA of Burns and contemporaries',\n",
    "#                 xaxis=XAxis(title='Principal Component 1'),\n",
    "#                 yaxis=YAxis(title='Principal Component 2'))\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "# py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from ete3 import Tree, TreeStyle\n",
    "# t = Tree('newck', format=1)\n",
    "# ts.show_leaf_name = True\n",
    "# ts.scale =  120 # 120 pixels per branch length unit\n",
    "# t.show(tree_style=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
