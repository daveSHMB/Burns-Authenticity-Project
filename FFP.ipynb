{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from io import open\n",
    "\n",
    "# set directory for text input files\n",
    "text_files = os.path.join(os.path.dirname(\"__file__\"), 'texts')\n",
    "files = os.listdir(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "authors = []\n",
    "\n",
    "# Loop through each text file and append relevant data to lists\n",
    "for doc in os.listdir(text_files):\n",
    "\n",
    "    text = open(text_files + '\\\\' + doc, 'r', encoding='utf-8')\n",
    "    # open the file, store as lower case\n",
    "    text = text.read().lower()\n",
    "    texts.append(text)\n",
    "    \n",
    "    authors.append(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_grams = set([])\n",
    "treated_texts = []\n",
    "\n",
    "import regex as re\n",
    "\n",
    "for text in texts:\n",
    "    \n",
    "    # Remove high frequency stop words, as in Sims/Wu et. al\n",
    "    stopwords = ['and', 'the', 'a', 'an']\n",
    "    #text = text.split()\n",
    "    \n",
    "    # removes all punctuation, spaces and new lines    \n",
    "    text = re.sub(ur\"\\p{P}+\", \"\", text)\n",
    "    \n",
    "    # Split text, remove stopwords and rejoin string without spaces or newlines\n",
    "    text = text.split()\n",
    "\n",
    "    text = [x for x in text if x not in stopwords]\n",
    "    \n",
    "    text = ''.join(text)   \n",
    "    \n",
    "    #produces 6 character n-grams in a 'sliding window' across the text\n",
    "    text = [text[i:i+9] for i in range(0, len(text), 1)]\n",
    "    treated_texts.append(text)\n",
    "    #n_gram_frequency = Counter(text)\n",
    "\n",
    "    # Append features to the ngram list\n",
    "    from collections import Counter\n",
    "    c = Counter(text)\n",
    "\n",
    "    # Append features to the ngram list\n",
    "    for key, val in c.iteritems():\n",
    "        if val > 1:\n",
    "            n_grams.add(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "n_grams = OrderedDict.fromkeys(n_grams, 0.0) \n",
    "\n",
    "frequencies = []\n",
    "\n",
    "t = []\n",
    "\n",
    "for text in treated_texts:\n",
    "    c = Counter(text)\n",
    "    temp = OrderedDict(n_grams)\n",
    "    \n",
    "    for key, val in c.iteritems():\n",
    "        if val > 1:\n",
    "            temp[key] = float(val)\n",
    "\n",
    "    frequencies.append(temp.values())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def JSD(P, Q):\n",
    "    P = P / norm(np.array(P), ord=1)\n",
    "    Q = Q / norm(np.array(Q), ord=1)\n",
    "    M = 0.5 * (P + Q)\n",
    "    jsd = (0.5 * entropy(P, M)) + (0.5 * entropy(Q, M))\n",
    "    return jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO - make this more efficient\n",
    "\n",
    "distance_matrix = [[] for x in range(len(frequencies))]\n",
    "\n",
    "for i in range(len(frequencies)):\n",
    "    for j in range(len(frequencies)):\n",
    "        \n",
    "        if i==j:\n",
    "            distance_matrix[i].append(0)\n",
    "            break\n",
    "        \n",
    "        distance_matrix[i].append(JSD(frequencies[i], frequencies[j]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            _____________________________ conrad 3.txt\n",
      "          ,|\n",
      "          ||____________________________ conrad 2.txt\n",
      "         _|\n",
      "        | |  _____________________________ twain 4.txt\n",
      "        | | |\n",
      "        | |_|   _____________________________ twain 2.txt\n",
      "        |   | ,|\n",
      "        |   | ||_________________________________ twain 1.txt\n",
      "        |   |_|\n",
      "      __|     |     ________________________________ shakespeare 1.txt\n",
      "     |  |     |____|\n",
      "     |  |          |__________________________ conrad 1.txt\n",
      "     |  |\n",
      "     |  |            _________________________________ shakespeare 3.txt\n",
      "     |  |    _______|\n",
      "     |  |   |       |_______________________________ shakespeare 2.txt\n",
      "    ,|  |___|\n",
      "    ||      |   __________________________________________ B-List 1.txt\n",
      "    ||      | _|\n",
      "    ||      || |_________________________________________ A-List 1.txt\n",
      "    ||       |\n",
      "    ||       |_________________________________________ all burns.txt\n",
      "    ||\n",
      "  __||    ____________________________ austen 2.txt\n",
      " |  ||___|\n",
      " |  |    | ________________________ austen 3.txt\n",
      " |  |    ||\n",
      " |  |     |___________________________ austen 1.txt\n",
      " |  |\n",
      " |  |  _____________________________ dickens 7.txt\n",
      " |  |_|\n",
      " |    |    _________________________ dickens 5.txt\n",
      " |    |___|\n",
      " |        |_________________________ dickens 4.txt\n",
      " |\n",
      "_| _________________________ scott 2.txt\n",
      " ||\n",
      " ||_________________________ scott 1.txt\n",
      " |\n",
      " |______________________________ scott 3.txt\n",
      " |\n",
      " |___________________________ scott 5.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Bio.Phylo.TreeConstruction import DistanceTreeConstructor\n",
    "from Bio.Phylo.TreeConstruction import _DistanceMatrix \n",
    "from Bio import Phylo\n",
    "\n",
    "m = _DistanceMatrix(authors, distance_matrix)\n",
    "\n",
    "\n",
    "constructor = DistanceTreeConstructor()\n",
    "\n",
    "njtree = constructor.nj(m)\n",
    "\n",
    "\n",
    "Phylo.draw_ascii(njtree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~daveSHMB/181.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # PCA using the ngram frequencies, just for laughs\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# ###### Convert both X and Y to numpy arrays\n",
    "# X = np.array(frequencies)\n",
    "# y = np.array(authors)\n",
    "# markers = set(authors)\n",
    "\n",
    "# # Standardise data, perform PCA\n",
    "# from sklearn.decomposition import PCA as sklearnPCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# X_std = StandardScaler().fit_transform(X)\n",
    "# sklearn_pca = sklearnPCA(n_components=2)\n",
    "# Y_sklearn = sklearn_pca.fit_transform(X_std)\n",
    "\n",
    "# import plotly.plotly as py\n",
    "# from plotly.graph_objs import *\n",
    "# import plotly.tools as tls\n",
    "\n",
    "# traces = []\n",
    "\n",
    "# for author in markers:\n",
    "\n",
    "#     trace = Scatter(\n",
    "#         x=Y_sklearn[y==author,0],\n",
    "#         y=Y_sklearn[y==author,1],\n",
    "#         mode='markers',\n",
    "#         name=author,\n",
    "#         marker=Marker(\n",
    "#             size=14))\n",
    "#     traces.append(trace)\n",
    "\n",
    "\n",
    "# data = Data(traces)\n",
    "# layout = Layout(title='PCA of Burns and contemporaries',\n",
    "#                 xaxis=XAxis(title='Principal Component 1'),\n",
    "#                 yaxis=YAxis(title='Principal Component 2'))\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "# py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
