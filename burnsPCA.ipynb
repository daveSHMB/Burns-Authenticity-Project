{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "from io import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set directory for text input files\n",
    "text_files = os.path.join(os.path.dirname(\"__file__\"), 'texts')\n",
    "files = os.listdir(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lists for storing data for PCA processing\n",
    "X = []  # Stores data on each input text\n",
    "y = []  # Stores authors of input texts\n",
    "markers = []  # Stores only unique author names for plotting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from numpy import std\n",
    "from collections import Counter\n",
    "\n",
    "def bigrams(words):\n",
    "    \"\"\"Calculates number of unique bigrams per chunk\"\"\"\n",
    "    from nltk import bigrams\n",
    "    bg = bigrams(words)\n",
    "    bg = set(bg)\n",
    "    bg = Counter(bg)\n",
    "\n",
    "    return (float(len(bg)) / len(words))\n",
    "    \n",
    "\n",
    "def mean_word_length(words):\n",
    "    \"\"\"Calculates mean length of words in a text\"\"\"\n",
    "    total_length = 0\n",
    "    for word in words:\n",
    "        total_length += len(word)\n",
    "    return total_length/len(words)\n",
    "\n",
    "\n",
    "def mean_sentence_length(sentences):\n",
    "    \"\"\"Calculates the mean length of each sentence\"\"\"\n",
    "    total_length = 0\n",
    "    for sentence in sentences:\n",
    "        total_length += len(sentence)\n",
    "\n",
    "    mean = total_length/len(sentences)\n",
    "    return mean\n",
    "\n",
    "\n",
    "def sd_of_sentence_length(sentences):\n",
    "    \"\"\"Returns the standard deviation in sentence length\"\"\"\n",
    "    sentence_lengths = []\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        sentence_lengths.append(sentence_length)\n",
    "\n",
    "    sd = std(sentence_lengths)\n",
    "    return sd\n",
    "\n",
    "\n",
    "def count_words(words, word_to_count):\n",
    "    \"\"\"Returns the count of a given word/character per chunk\"\"\"\n",
    "    total = 0\n",
    "    \n",
    "    for word in words:\n",
    "        total += word.count(word_to_count)\n",
    "    \n",
    "    #total += words.count(word_to_count)\n",
    "       \n",
    "    return (float(total) / len(words))\n",
    "\n",
    "\n",
    "def type_token_ratio(words):\n",
    "    \"\"\"Calculates the type token ratio of a text\"\"\"\n",
    "    c = Counter(words)\n",
    "    # TTR is the number of unique words in a text divided by the total word count\n",
    "    #ttr = float(len(c)) / len(words)\n",
    "    ttr = float(len(c)) / len(words)\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_elems = [',', ';', '\"', '!', '-', 'and', 'but', 'however',\n",
    "                'if', 'that', 'more', 'must', 'might', 'this', 'very',\n",
    "                'ye', 'wi', 'thou', 'thy', 'may', 'man', 'sae', 'like',\n",
    "               'thee', 'heart', 'love', 'day']\n",
    "\n",
    "# Loop through each text file and append relevant data to lists\n",
    "for doc in os.listdir(text_files):\n",
    "\n",
    "    text = open(text_files + '\\\\' + doc, 'r', encoding='utf-8')\n",
    "    # open the file, store as lower case\n",
    "    text = text.read().lower()\n",
    "    \n",
    "    #get the author from the title (in format <Author> <volume no>.txt)\n",
    "    author = doc.split()\n",
    "    author = author[0].title()\n",
    "    \n",
    "    # if author isn't already in list of markers, add it\n",
    "    if author not in markers:\n",
    "        markers.append(author)\n",
    "    \n",
    "    \n",
    "    words = text.split()\n",
    "    sentences = sent_tokenize(text)\n",
    "    # split into chunks of 5000 words\n",
    "    words = [words[i:i+5000] for i in range(0, len(words), 5000)]\n",
    "    \n",
    "    \n",
    "    full_text_data = [] # Array for storing all chunk data lists\n",
    "    \n",
    "    for chunk in words:\n",
    "        chunk_data = [] # Array for storing the data gathered from this chunk\n",
    "        chunk_data.append(mean_word_length(chunk))\n",
    "        chunk_data.append(bigrams(chunk))\n",
    "        chunk_data.append(type_token_ratio(chunk))\n",
    "        #chunk_data.append(mean_sentence_length(sentences))\n",
    "        #chunk_data.append(sd_of_sentence_length(sentences))\n",
    "        \n",
    "        for w in common_elems:\n",
    "            chunk_data.append(count_words(chunk, w))\n",
    "        \n",
    "        X.append(chunk_data)\n",
    "        #add author of chunk to y\n",
    "        y.append(author)\n",
    "        \n",
    "    \n",
    "    #X.append(full_text_data)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### Convert both X and Y to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardise data, perform PCA\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~daveSHMB/177.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.tools as tls\n",
    "\n",
    "traces = []\n",
    "\n",
    "for author in markers:\n",
    "\n",
    "    trace = Scatter(\n",
    "        x=Y_sklearn[y==author,0],\n",
    "        y=Y_sklearn[y==author,1],\n",
    "        mode='markers',\n",
    "        name=author,\n",
    "        marker=Marker(\n",
    "            size=14))\n",
    "    traces.append(trace)\n",
    "\n",
    "\n",
    "data = Data(traces)\n",
    "layout = Layout(title='PCA of Burns and contemporaries',\n",
    "                xaxis=XAxis(title='Principal Component 1'),\n",
    "                yaxis=YAxis(title='Principal Component 2'))\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.figure(figsize=(18, 7), dpi=96)\n",
    "\n",
    "    # Amend this so a random but unique colour is selected for each author\n",
    "    for lab, col in zip(markers, ('blue', 'red', 'green', 'black', 'orange', 'pink', 'brown')):\n",
    "\n",
    "        plt.scatter(Y_sklearn[y == lab, 0],\n",
    "                    Y_sklearn[y == lab, 1],\n",
    "                    label=lab,\n",
    "                    c=col,\n",
    "                    s=40)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(loc='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
